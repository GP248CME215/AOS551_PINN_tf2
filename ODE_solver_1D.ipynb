{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE import lhs\n",
    "import time\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class funcODE_PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x_u, u, x_f, gov_eqn, layers, lyscl, lb, ub, gamma):\n",
    "        \n",
    "        self.x_u = x_u\n",
    "        self.u = u\n",
    "        self.x_f = x_f\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "                \n",
    "        self.layers = layers\n",
    "        self.lyscl = lyscl\n",
    "        self.lw = gamma\n",
    "        \n",
    "        self.data_type = x_f.dtype\n",
    "        \n",
    "        self.gov_eqn = gov_eqn\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers, lyscl)\n",
    "\n",
    "        \n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be \n",
    "        #            automatically updated in the original tf.Variable\n",
    "        \n",
    "        # define the loss function\n",
    "        self.loss = self.loss_NN()[0]\n",
    "        \n",
    "        self.optimizer_Adam = tf.optimizers.Adam()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Functions used to establish the initial neural network\n",
    "    ===============================================================\n",
    "    '''\n",
    "    \n",
    "    def initialize_NN(self, layers, lyscl):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)       \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.MPL_init(size=[layers[l], layers[l+1]], lsnow=lyscl[l])                \n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=self.data_type))\n",
    "            weights.append(W)\n",
    "            biases.append(b)  \n",
    "        return weights, biases\n",
    "    \n",
    "    \n",
    "    def MPL_init(self, size, lsnow):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim)) * lsnow\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=self.data_type))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Functions used to building the physics-informed contrainst and loss\n",
    "    ===============================================================\n",
    "    '''\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def net_u(self, x):\n",
    "        u = self.neural_net(x, self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    \n",
    "    def net_f(self, x):\n",
    "        f = self.gov_eqn(x, self.net_u)\n",
    "        return f\n",
    "    \n",
    "    \n",
    "    # calculate the physics-informed loss function\n",
    "    def loss_NN(self):\n",
    "        self.u_pred = self.net_u(self.x_u) \n",
    "        self.f_pred = self.net_f(self.x_f)  \n",
    "        \n",
    "        loss_d = tf.reduce_mean(tf.square(self.u - self.u_pred))\n",
    "        loss_e = tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "        loss = loss_d + self.lw * loss_e\n",
    "        return loss, loss_d, loss_e\n",
    "    \n",
    "    \n",
    "\n",
    "    def net_u_test(self, x):\n",
    "        u_ts = self.net_u(x) + self.r0 * self.net_f(x)    \n",
    "        return u_ts\n",
    "    \n",
    "    \n",
    "    def net_f_test(self, x, epsil):\n",
    "        self.r0 = epsil  \n",
    "        f_ts = self.gov_eqn(x, self.net_u_test)\n",
    "        return f_ts\n",
    "    \n",
    "   \n",
    "    \n",
    "    '''\n",
    "    Functions used to define ADAM optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "    # define the function to apply the ADAM optimizer\n",
    "    def Adam_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.optimizer_Adam.minimize(self.loss_NN, varlist)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.loss_NN()[0]\n",
    "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
    "                      (it, loss_value, elapsed))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                \n",
    "    '''\n",
    "    Functions used to define L-BFGS optimizers\n",
    "    ===============================================================\n",
    "    '''\n",
    "    \n",
    "    # A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    def Lbfgs_function(self, varlist):\n",
    "        # obtain the shapes of all trainable parameters in the model\n",
    "        shapes = tf.shape_n(varlist)\n",
    "        n_tensors = len(shapes)\n",
    "    \n",
    "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "        # prepare required information first\n",
    "        count = 0\n",
    "        idx = [] # stitch indices\n",
    "        part = [] # partition indices\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "    \n",
    "        for i, shape in enumerate(shapes):\n",
    "            n = np.product(shape)\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "            part.extend([i]*n)\n",
    "            count += n\n",
    "    \n",
    "        part = tf.constant(part)\n",
    "        \n",
    "        def assign_new_model_parameters(params_1d):\n",
    "            # A function updating the model's parameters with a 1D tf.Tensor.\n",
    "            # Sub-function under function of class not need to input self\n",
    "            \n",
    "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                varlist[i].assign(tf.reshape(param, shape))\n",
    "                \n",
    "                \n",
    "        @tf.function\n",
    "        def f(params_1d):\n",
    "            # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "            # This function is created by function_factory.\n",
    "            # Sub-function under function of class not need to input self\n",
    "    \n",
    "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "            with tf.GradientTape() as tape:\n",
    "                # update the parameters in the model \n",
    "                # this step is critical for self-defined function for L-BFGS\n",
    "                assign_new_model_parameters(params_1d)\n",
    "                # calculate the loss\n",
    "                loss_value, loss_d, loss_e = self.loss_NN()\n",
    "\n",
    "    \n",
    "            # calculate gradients and convert to 1D tf.Tensor\n",
    "            grads = tape.gradient(loss_value, varlist)\n",
    "            grads = tf.dynamic_stitch(idx, grads)\n",
    "    \n",
    "            # store loss value so we can retrieve later\n",
    "            tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "                \n",
    "            # print out iteration & loss\n",
    "            f.iter.assign_add(1)\n",
    "            \n",
    "            if f.iter % 10 == 0:\n",
    "                tf.print(\"Iter:\", f.iter, \"loss:\", loss_value,\n",
    "                         \"loss_d:\", loss_d, \"loss_e:\",loss_e)\n",
    "                \n",
    "            return loss_value, grads\n",
    "        \n",
    "        # store these information as members so we can use them outside the scope\n",
    "        f.iter = tf.Variable(0)\n",
    "        f.idx = idx\n",
    "        f.part = part\n",
    "        f.shapes = shapes\n",
    "        f.assign_new_model_parameters = assign_new_model_parameters\n",
    "        f.history = []\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    \n",
    "    # define the function to apply the L-BFGS optimizer\n",
    "    def Lbfgs_optimizer(self, nIter):\n",
    "        varlist = self.train_variables\n",
    "        func = self.Lbfgs_function(varlist)\n",
    "    \n",
    "        # convert initial model parameters to a 1D tf.Tensor\n",
    "        init_params = tf.dynamic_stitch(func.idx, varlist)\n",
    "        \n",
    "        max_nIter = tf.cast(nIter/3, dtype = tf.int32)\n",
    "    \n",
    "        # train the model with L-BFGS solver\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            value_and_gradients_function=func, initial_position=init_params, \n",
    "            tolerance=10e-9, max_iterations=max_nIter)\n",
    "    \n",
    "        # after training, the final optimized parameters are still in results.position\n",
    "        # so we have to manually put them back to the model\n",
    "        func.assign_new_model_parameters(results.position)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Function used for training the model\n",
    "    ===============================================================\n",
    "    '''\n",
    "        \n",
    "    def train(self, nIter, idxOpt):\n",
    "        if idxOpt == 1:\n",
    "            # mode 1: running the Adam optimization\n",
    "            self.Adam_optimizer(nIter)\n",
    "        elif idxOpt == 2:\n",
    "            self.Lbfgs_optimizer(nIter)\n",
    "            \n",
    "        \n",
    "    def predict(self, x):\n",
    "        u_p = self.net_u(x)\n",
    "        f_p = self.net_f(x)  \n",
    "        return u_p, f_p\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# define the decorator for calculating the derivative with x\n",
    "def fwgradient(func):\n",
    "    def wrapper(x):\n",
    "        with tf.GradientTape() as tg:\n",
    "            tg.watch(x) #must add watch(), since x is not tf.Variable\n",
    "            U = func(x)\n",
    "        g = tg.gradient(U,x)\n",
    "        return g\n",
    "    return wrapper   \n",
    "\n",
    "\n",
    "# define the governing equation to solve\n",
    "def gov_eqn(x, func_u):\n",
    "    # using the decorator to define the batch jacobian        \n",
    "    ux_func = fwgradient(func_u)\n",
    "\n",
    "    u = func_u(x)    \n",
    "    u_x = ux_func(x)\n",
    "    f = u_x - u\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "     \n",
    "    '''\n",
    "    Define the hyper-parameter\n",
    "    ============================================\n",
    "    '''\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1243)\n",
    "    \n",
    "    N_f = 401\n",
    "    N_t = 101\n",
    "    layers = [1,20,1]\n",
    "    lyscl = [1,1]\n",
    "    \n",
    "    data_type = tf.float32\n",
    "    \n",
    "    '''\n",
    "    Define the problem\n",
    "    ============================================\n",
    "    '''\n",
    "    x_train = tf.constant([0.], dtype=data_type)[:,None]\n",
    "    u_train = tf.constant([1.], dtype=data_type)[:,None]\n",
    "    x_star = np.linspace(0,2,N_f)[:,None]\n",
    "    u_star = np.exp(x_star)\n",
    "    \n",
    "    # Doman bounds\n",
    "    lx = x_star.min()\n",
    "    ux = x_star.max()\n",
    "        \n",
    "    '''\n",
    "    Conducting the first-round prediction\n",
    "    ============================================\n",
    "    '''\n",
    "    x_f_train = np.linspace(lx,ux,N_t)[:,None]\n",
    "    x_f_train = lx + (ux-lx)*lhs(1, N_t)\n",
    "    \n",
    "    x_f_train = np.vstack((x_f_train, x_train))\n",
    "    x_f_train = tf.cast(x_f_train, dtype=data_type)\n",
    "    \n",
    "    gamma = 10\n",
    "\n",
    "    model = funcODE_PINN(x_train, u_train, x_f_train, gov_eqn, layers, lyscl, lx, ux, gamma)\n",
    "    start_time = time.time()        \n",
    "    model.train(10000,2)\n",
    "    elapsed = time.time() - start_time                \n",
    "    print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "    x_star = tf.cast(x_star, dtype=data_type)   \n",
    "    u_pred, f_pred = model.predict(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ######################################################################\n",
    "    ############################# Plotting ###############################\n",
    "    ######################################################################    \n",
    "    \n",
    "    fig = plt.figure(figsize = [10, 10], dpi = 300)\n",
    "    \n",
    "    ax = plt.subplot(211)\n",
    "    ax.plot(x_star, u_star, 'b-', linewidth = 2, label = 'exact')\n",
    "    ax.plot(x_star, u_pred, 'r--', linewidth = 2, label = 'predict')\n",
    "\n",
    "    ax.set_xlabel('$x$', fontsize = 12)\n",
    "    ax.set_ylabel('$u$', fontsize = 12, rotation = 0)\n",
    "    ax.set_title('solution', fontsize = 15)\n",
    "\n",
    "    \n",
    "    ax = plt.subplot(212)\n",
    "    ax.plot(x_star, f_pred, 'b-', linewidth = 2)\n",
    "\n",
    "    ax.set_xlabel('$x$', fontsize = 12)\n",
    "    ax.set_ylabel('$f$', fontsize = 12, rotation = 0)\n",
    "    ax.set_title('equation residue', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
